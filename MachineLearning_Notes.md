[TOC]



### Chap1. Feature Engineering

对于机器学习，数据和特征决定了结果的上限，模型、算法的选择及优化则是在逐步接近这个上限。

特征工程本质：去除原始数据中的杂质和冗余，设计更高效的特征以描述问题与预测模型间的关系。

2种数据：

​	结构化数据：关系数据库中的表。

​	非结构化数据：文本，图像，音频，视频。

#### **01 特征归一化 Normalization**

消除数据特征间的量纲，对特征进行归一化处理，使不同指标间有可比性。

**❀特征归一化**

#### Q: 为什么需要对数值型的特征做归一化?

A: 可以将所有的特征都统一到一个大致相同的数值区间内。

比如有两个数值型特征，X1∈[0,10]，X2∈[0,3]。采用随机梯度下降，不归一化的情况下，在学习率相同时，X1的更新速度会大于X2，需要多次迭代才能找到最优解。使用归一化，X1和X2的的更新速度变为更一致，容易更快地通过梯度下降找到最优解。

常用的方法有：

​	(1) 线性函数归一化 Min-Max Scaling: 对原始数据进行线性变换，使结果映射到[0,1]范围，算一种等比缩放。
$$
Xnorm = \frac{X - Xmin}{Xmax - Xmin}
$$
​	(2) 零均值归一化 Z-Score Normalization: 将原始数据映射到均值=0，标准差=1的分布上。
$$
z = \frac{x-\mu }{\sigma }
$$
注：一般通过梯度下降求解的模型通常需要归一化，包括线性回归，逻辑回归，SVM，神经网络等模型。但是，决策树模型并不适用，C4.5: 决策树在进行节点分裂时主要依据数据集关于特征的信息增益比，信息增益比跟特征是否经过归一化是无关的。





#### **02 类别型特征 Categorical Feature**

通常指在优先选项内取值的特征，性别，血型等。类别型特征的原始输入通常是字符串形式，除了决策树等少数模型可以直接处理字符串形式的输入，对于逻辑回归，SVM等模型，必须先将类别型特征转换成数值型特征才可。

**❀序号编码 Ordinal Encoding，独热编码 One-hot Encoding， 二进制编码 Binary Encoding**

#### Q: 在对数据预处理时，该怎样处理类别型特征?

A: 序号编码: 通常用于处理具有大小关系且排序的数据。成绩，高-3，中-2，低-1。

​    独热编码: 处理类别间不具有大小关系的特征。血型，A-(1,0,0,0)，B-(0,1,0,0)，C-(0,0,1,0)，D-(0,0,0,1)。注：当类别取值较多的情况下，(1) 使用稀疏向量来节省空间。目前大多数算法均支持稀疏向量形式的输入。(2) 配合特征选择来降低纬度。高维度特征一般存在的问题有: K-近邻，高维空间两点间的距离很难有效衡量；逻辑回归模型，参数的数量随维度的增高而增加，容易过拟合；一般只有部分维度是对分类、预测有帮助的。

​    二进制编码: 先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。**本质**是利用二进制对ID进行哈希映射，得到0/1特征向量，维度少于独热编码，节省存储空间。





#### **03 高维组合特征**

**❀组合特征**

#### Q: 什么是组合特征？如何处理高维组合特征？

A: 为了提高复杂关系的拟合能力，特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种离散特征。可以将语言和类型组成二阶特征。





#### **04 组合特征**

面对多种高维特征，如果只简单地两两组合，依然容易存在参数过多，过拟合的问题，且并非所有的特征组合都是有意义的。

**❀组合特征**

#### Q: 怎样有效地找到组合特征？





#### **05 文本表示模型**

**❀词袋模型 Bag of Words，TF-IDF Term Frequency-Inverse Document Frequency，主题模型 Topic Model，词嵌入模型 Word Embedding**

#### Q: 有哪些文本表示模型？有什么优缺点？

A: **词袋模型和N-gram模型**: 词袋模型是最基础的文本表示模型。每篇文章看成一袋子词，忽略每个词出现的顺序。文章可以表示为一个长向量，向量中的每一个维度代表一个单词，该维度对应的权重反映了这个词在原文中的重要程度。用TF-IDF来计算权重
$$
TF-IDF(t,d) = TF(t,d)×IDF(t)
$$
 TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性，
$$
IDF(t) = log\frac{文章总数 }{包含t的文章总数+1}
$$
若一个单词在很多的文章里都出现，那么它可能是个通用词汇，对于区分某篇文章特殊语义贡献较小，重要性就会被削弱。

但是，将文章按照单词划分也不是很好的办法。拆分后的单个词与原来连续的词含义差别很大。通常，可以将连续出现的n个词组成的词组作为一个单的的特征放到向量表示中去，即N-gram模型。

​     **主题模型**: 从文本库中发现有代表性的主题(得到每个主题上面词的分布特性)，且能够计算出每篇文章的主题分布。

​    **词嵌入与深度学习模型**: 词嵌入是一类将词向量化的模型统称。主要是将每个词都映射成低维空间K(K=50~300)上的一个dense vector。K中每一维空间可看做是一个隐含的主题。

**注**: 在传统的浅层机器学习模型中，一个好的特征工程可以带来算法效果的提升。而深度学习模型为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。**卷积和循环神经网络**在文本表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取一些高层的语义特征。**与全连接的网络结构**相比，卷积和循环神经网络一方面抓住了文本的特性，另一方面减少了网络中待学习的参数，提高训练速度，降低过拟合的风险。





#### 06 Word2Vec

由Google提出的一种常用的词嵌入模型之一。Word2Vec实际是一种浅层神经网络模型，有两种网络结构: CBOW Continues Bag of Words和 Skip-gram。

**❀Word2Vec，隐狄利克雷模型LDA，CBOW，Skip-gram**

#### Q: Word2Vec如何工作？它和LDA有什么区别与联系？

A: CBOW是根据上下文出现的词语来预测当前词的生成概率，

​    Skip-gram是根据当前词来预测上下文中各词的生成概率。

​    CBOW，Skip-gram都可以表示成由输入层，映射层，输出层组成的神经网络。

​    LDA是利用文档中单词的共现关系来对单词按主题聚类，即对文档-单词矩阵进行分解，得到文档-主题和主题-单词两个概率分布。Word2Vec是对上下文-单词矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。若2个单词对应的Word2Vec向量相似度高，那么他们很可能经常在同样的上下文中出现。

  主题模型和词嵌入模型两类的区别在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，包括需要推测的隐含变量(主题)；词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的dense vector。





#### 07 图像数据不足时的处理**

**❀迁移学习，GAN，图像处理，上采样，数据扩充**

#### Q: 图像分类任务中，训练数据不足会带来什么问题？如何缓解数据不足的问题？

A: 主要问题是过拟合，在测试集上泛化效果不佳。处理方法：

​	(1) 基于模型，采用降低过拟合风险的措施，包括简化模型(非线性模型简化为线性)，添加约束以缩小假设空间(L1/L2正则)，集成学习，dropout超参数等。

​	(2) 基于数据，通过先验知识，在保持特定信息的前提下，用数据扩充。一定程度内的随机旋转，平移，缩放，剪裁，翻转等。这些变换对应同一个目标在不同角度的观察结果。也可以在像素中增加噪声，椒盐噪声，高斯白噪声等。颜色变换，改变亮度，清晰度，对比度，锐度等。

​	(3) 对图像进行特征提取，在图像的特征空间进行变换，利用数据扩充或者上采样。

​	(4) 使用生成模型来合成一些新样本，GAN。

​	(5) 迁移学习：借用一个在大规模数据集上训练好的通用模型，并在针对目标任务的小数集上fine-tune。



